import duckdb, os, boto3, urllib.parse

bucket = os.environ["EXPLOITATION_ZONE_BUCKET"]
endpoint = os.environ["MINIO_ENDPOINT"]  # e.g. http://minio:9000
access = os.environ["MINIO_ROOT_USER"]
secret = os.environ["MINIO_ROOT_PASSWORD"]


s3 = boto3.client(
    "s3",
    endpoint_url=endpoint,
    aws_access_key_id=access,
    aws_secret_access_key=secret,
)

paginator = s3.get_paginator("list_objects_v2")
pages = paginator.paginate(Bucket=bucket, Prefix="_staging_parquet/", Delimiter="/")

folders = []
for page in pages:
    # CommonPrefixes holds "folders" for Delimiter="/"
    for prefix in page.get("CommonPrefixes", []):
        # strip trailing slash
        name = prefix["Prefix"].rstrip("/").split("/")[-1]
        folders.append(name)

# geting rid of the http scheme so DuckDB gets host:port only
host = urllib.parse.urlparse(endpoint).netloc
db_file = "/tmp/exploitation.duckdb"
con = duckdb.connect(db_file)
con.execute("INSTALL httpfs; LOAD httpfs;")

con.sql(
    f"""
    SET s3_endpoint='{host}';
    SET s3_access_key_id='{access}';
    SET s3_secret_access_key='{secret}';
    SET s3_url_style='path';
    SET s3_use_ssl=false;
"""
)

# import duckdb, os, boto3, urllib.parse

# bucket = os.environ["MINIO_DATA_BUCKET"]
# endpoint = "minio:9000"  # e.g. minio:9000
# access = os.environ["MINIO_ROOT_USER"]
# secret = os.environ["MINIO_ROOT_PASSWORD"]

# # strip scheme so DuckDB gets host:port only
# host = urllib.parse.urlparse(endpoint).netloc

# db_file = "/tmp/exploitation.duckdb"
# con = duckdb.connect(db_file)
# con.execute("INSTALL httpfs; LOAD httpfs;")


# con.execute(
#     f"""
#     CREATE OR REPLACE SECRET minio_secret (
#       TYPE s3,
#       PROVIDER config,
#       KEY_ID    '{access}',
#       SECRET    '{secret}',
#       ENDPOINT  '{host.removeprefix("http://")}',
#       REGION    'us-east-1',
#       URL_STYLE 'path',
#       USE_SSL   false
#     );
#     """
# )

# ingest parquet â€” be sure the prefix matches the Spark export!
for name in folders:
    con.sql(
        f"""
        CREATE OR REPLACE TABLE {name} AS
        SELECT * FROM parquet_scan(
            's3://{bucket}/_staging_parquet/{name}/*.parquet',
            filename=false, hive_partitioning=true
        );
    """
    )
    result = con.sql(f"SELECT * FROM {name}").df()
    print(result)

con.sql("CHECKPOINT")
con.commit()
con.close()


# upload the DB file to MinIO (overwriting)
s3.upload_file(db_file, bucket, f"exploitation.duckdb")
