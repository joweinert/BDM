"""
Daily trusted‑zone → Parquet → DuckDB snapshot builder.
"""
# ok now we need to enrich the data and bring it into datamarts.

# Its example data for a uni project so its not too much and too amazing but ill provide you with a schema overview from my trusted zone and the script that dumps th data in parquet.

# Another script makes a duckdb databank from these parquets. Now I want the dump script to enhance data merge schemas in a way that we have different duckdbs exemplifying datamarts so idk, make up some roles and what data might be interesting for them, then write the spark code to make the parquets with this new enriched schemas, also define kpis we can get from the data and add these somewhere
from airflow import DAG
from airflow.providers.docker.operators.docker import DockerOperator
from datetime import datetime
from util.spark_manager import SparkManager

spark_mgr = SparkManager()

DEFAULT_ENV = {
    "MINIO_DATA_BUCKET": "deltalake",
    "MINIO_ENDPOINT": "http://minio:9000",
    "MINIO_ROOT_USER": "admin",
    "MINIO_ROOT_PASSWORD": "admin123",
    "EXPLOITATION_ZONE_BUCKET": "exploitation-zone",
}

with DAG(
    dag_id="build_exploitation_duckdb",
    start_date=datetime(2025, 5, 7),
    schedule_interval="@daily",
    catchup=False,
    tags=["exploitation"],
) as dag:

    export_parquet = spark_mgr.submit_spark_job(
        dag=dag,
        task_id="export_trusted_to_parquet",
        application="exploitation_zone/spark_export_trusted.py",  # path inside Spark image
    )

    build_snapshot = DockerOperator(
        task_id="build_duckdb_snapshot",
        image="snapshot_builder:latest",
        api_version="auto",
        mount_tmp_dir=False,
        network_mode="bdm_default",
        auto_remove="success",
        docker_url="unix://var/run/docker.sock",
        environment=DEFAULT_ENV,
    )

    export_parquet >> build_snapshot
