services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    restart: always
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    restart: always
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"  # Internal (Docker) access
      - "29092:29092"  # External (Host) access
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 1s
      retries: 10
      start_period: 5s

  producer:
    build: ./streaming
    container_name: kafka_producer
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - DOCKER_ENV=true
    command: ["python", "src/producer/kafka_producer.py"]

  consumer:
    build: ./streaming
    container_name: kafka_consumer
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - DOCKER_ENV=true
    command: ["python", "src/consumer/kafka_consumer.py"]

  postgres:
    image: postgres:latest
    container_name: airflow_postgres
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow:
    image: apache/airflow:latest
    container_name: airflow
    restart: always
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__WEBSERVER__DEFAULT_USER=username
      - AIRFLOW__WEBSERVER__DEFAULT_PASSWORD=password
    ports:
      - "8080:8080"  # Airflow UI
    volumes:
      - ./batch/src/dags:/opt/airflow/dags
      - ./batch/airflow_logs:/opt/airflow/logs
    entrypoint: /bin/bash -c "airflow db init && airflow users create --role Admin --username admin --password admin --firstname Air --lastname Flow --email admin@example.com && airflow scheduler & airflow webserver"

  delta-spark:
    image: deltaio/delta-docker:latest
    container_name: delta-spark
    restart: always
    ports:
      - "4040:4040"  # Spark UI
      - "8888:8888"  # Jupyter Notebook
    volumes:
      - ./batch/delta/notebooks:/home/jovyan/work  # Jupyter notebooks for batch processing
      - ./batch/delta/data:/data  # Data storage for Delta Lake
      - ./batch/src/pipelines:/scripts # pipeline scripts -> batch ingestion scripts triggered by dags in airflow 
    environment:
      - NOTEBOOK_ARGS=--NotebookApp.token=''  # Disable Jupyter token for easier access


volumes:
  postgres_data: