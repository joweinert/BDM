x-common: &common-settings
  environment:
    # Pass all environment variables from .env to each service
    &env-vars
    DOCKER_ENV: "true"
    BIGDATA_MODE: ${BIGDATA_MODE}
    # Airflow Database
    POSTGRES_USER: ${POSTGRES_USER}
    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    POSTGRES_DB: ${POSTGRES_DB}
    # Airflow Admin Credentials
    AIRFLOW_ADMIN_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
    AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
    AIRFLOW_ADMIN_FIRSTNAME: ${AIRFLOW_ADMIN_FIRSTNAME}
    AIRFLOW_ADMIN_LASTNAME: ${AIRFLOW_ADMIN_LASTNAME}
    AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL}
    # Spark
    SPARK_MASTER_URL: ${SPARK_MASTER_URL}
    # MinIO
    MINIO_ENDPOINT: ${MINIO_ENDPOINT}
    MINIO_ROOT_USER: ${MINIO_ROOT_USER}
    MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    MINIO_FIRST_TEAR_ANALYST_USER: ${MINIO_FIRST_TEAR_ANALYST_USER}
    MINIO_FIRST_TEAR_ANALYST_PASSWORD: ${MINIO_FIRST_TEAR_ANALYST_PASSWORD}
    MINIO_SECOND_TEAR_ANALYST_USER: ${MINIO_SECOND_TEAR_ANALYST_USER}
    MINIO_SECOND_TEAR_ANALYST_PASSWORD: ${MINIO_SECOND_TEAR_ANALYST_PASSWORD}
    MINIO_DATA_BUCKET: ${MINIO_DATA_BUCKET}
    MINIO_UNSTRUCTURED_BUCKET: ${MINIO_UNSTRUCTURED_BUCKET}
    EXPLOITATION_ZONE_BUCKET: ${EXPLOITATION_ZONE_BUCKET}
    # Kafka
    KAFKA_BROKER: ${KAFKA_BROKER}
    # Operations Database
    OPS_DB_HOST: ${OPS_DB_HOST}
    OPS_DB_PORT: ${OPS_DB_PORT}
    OPS_DB_NAME: ${OPS_DB_NAME}
    OPS_DB_USER: ${OPS_DB_USER}
    OPS_DB_PASS: ${OPS_DB_PASS}
    # Finnhub API
    FINNHUB_API_KEY: ${FINNHUB_API_KEY}

services:
  postgres:
    <<: *common-settings
    image: postgres:latest
    container_name: airflow_postgres
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    environment:
      <<: *env-vars
    volumes:
      - postgres-airflow_metadata:/var/lib/postgresql/data

  airflow-init:
    <<: *common-settings
    image: apache/airflow:2.7.3
    container_name: airflow_init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      <<: *env-vars
      _AIRFLOW_DB_MIGRATE: "true"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
      _AIRFLOW_WWW_USER_FIRSTNAME: ${AIRFLOW_ADMIN_FIRSTNAME}
      _AIRFLOW_WWW_USER_LASTNAME: ${AIRFLOW_ADMIN_LASTNAME}
      _AIRFLOW_WWW_USER_EMAIL: ${AIRFLOW_ADMIN_EMAIL}
      _AIRFLOW_WWW_USER_ROLE: Admin
    command: airflow version

  airflow-scheduler:
    <<: *common-settings
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    container_name: airflow-scheduler
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: scheduler
    environment:
      <<: *env-vars
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./src/util:/opt/airflow/util
      - ./logs/airflow:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock

  airflow-webserver:
    <<: *common-settings
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    container_name: airflow-webserver
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: webserver
    ports:
      - "8080:8080"
    environment:
      <<: *env-vars
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./src/util:/opt/airflow/util
      - ./logs/airflow:/opt/airflow/logs

  minio:
    <<: *common-settings
    build:
      context: .
      dockerfile: docker/minio/Dockerfile
    container_name: minio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: always
    environment:
      <<: *env-vars
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio_data:/data

  spark-master:
    <<: *common-settings
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    container_name: spark-master
    environment:
      <<: *env-vars
      SPARK_WORKER_MEMORY: 2g
      SPARK_DRIVER_MEMORY: 2g
      SPARK_EXECUTOR_MEMORY: 2g
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_SSL_ENABLED: no
      SPARK_USER: spark
    ports:
      - "7077:7077"
      - "8081:8080"
    volumes:
      - ./src/scripts:/opt/bitnami/spark/scripts
      - ./src/util:/opt/bitnami/spark/util
      - ./logs:/opt/bitnami/spark/logs
      - ./gx:/opt/bitnami/spark/gx

  spark-worker-1:
    <<: *common-settings
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    container_name: spark-worker-1
    environment:
      <<: *env-vars
      SPARK_WORKER_MEMORY: 2g
      SPARK_DRIVER_MEMORY: 2g
      SPARK_EXECUTOR_MEMORY: 2g
      SPARK_MODE: worker
      SPARK_USER: spark
    depends_on:
      - spark-master
    volumes:
      - ./src/scripts:/opt/bitnami/spark/scripts
      - ./src/util:/opt/bitnami/spark/util
      - ./logs:/opt/bitnami/spark/logs
      - ./gx:/opt/bitnami/spark/gx

  spark-worker-2:
    <<: *common-settings
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    container_name: spark-worker-2
    environment:
      <<: *env-vars
      SPARK_WORKER_MEMORY: 2g
      SPARK_DRIVER_MEMORY: 2g
      SPARK_EXECUTOR_MEMORY: 2g
      SPARK_MODE: worker
      SPARK_USER: spark
    depends_on:
      - spark-master
    volumes:
      - ./src/scripts:/opt/bitnami/spark/scripts
      - ./src/util:/opt/bitnami/spark/util
      - ./logs:/opt/bitnami/spark/logs
      - ./gx:/opt/bitnami/spark/gx

  zookeeper:
    <<: *common-settings
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    restart: always
    environment:
      <<: *env-vars
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    <<: *common-settings
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    restart: always
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      <<: *env-vars
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 3s
      retries: 30
      start_period: 5s

  app_backend:
    build:
      context: .
      dockerfile: docker/app_backend/Dockerfile
    container_name: app_backend
    ports:
      - "6969:6969"
    depends_on:
      minio:
        condition: service_healthy
      kafka:
        condition: service_healthy
    environment:
      <<: *env-vars
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:6969/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    command: uvicorn src.app_backend:app --host 0.0.0.0 --port 6969

  user_simulation:
    build:
      context: .
      dockerfile: docker/user_simulation/Dockerfile
    container_name: user_simulation
    environment:
      BIGDATA_MODE: ${BIGDATA_MODE}
    depends_on:
      app_backend:
        condition: service_healthy
    volumes:
      - ./assets/images:/app/images
      - ./assets/pdfs:/app/pdfs
      - ./.kaggle:/root/.kaggle
    command: python src/user_simulation.py

  operations_db:
    image: postgres:latest
    container_name: operations_db
    restart: always
    environment:
      <<: *env-vars
      POSTGRES_USER: ${OPS_DB_USER}
      POSTGRES_PASSWORD: ${OPS_DB_PASS}
      POSTGRES_DB: ${OPS_DB_NAME}
    ports:
      - "${OPS_DB_PORT}:${OPS_DB_PORT}"
    volumes:
      - ./data/postgres-ops-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  stock_producer:
    <<: *common-settings
    build:
      context: .
      dockerfile: docker/kafka/Dockerfile
    container_name: stock_producer
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      <<: *env-vars
      DOCKER_ENV: "true"
    command: ["python", "streaming/producer/stock_producer.py"]

  crypto_producer:
    <<: *common-settings
    build:
      context: .
      dockerfile: docker/kafka/Dockerfile
    container_name: crypto_producer
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      <<: *env-vars
      DOCKER_ENV: "true"
    command: ["python", "streaming/producer/crypto_producer.py"]

  # consumer:
  #   <<: *common-settings
  #   build:
  #     context: .
  #     dockerfile: docker/kafka/Dockerfile
  #   container_name: kafka_consumer
  #   restart: always
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   environment:
  #     <<: *env-vars
  #     DOCKER_ENV: "true"
  #   command: ["python", "streaming/consumer/simple_consumer.py"]

  snapshot_builder:
    <<: *common-settings
    image: snapshot_builder:latest
    build:
      context: .
      dockerfile: docker/snapshot_builder/Dockerfile
    container_name: snapshot_builder
    profiles: ["builder"]
    environment:
      <<: *env-vars
      DOCKER_ENV: "true"
    command: ["tail", "-f", "/dev/null"]

  # flink-jobmanager:
  #   <<: *common-settings
  #   build:
  #     context: .
  #     dockerfile: docker/flink/Dockerfile
  #   container_name: flink-jobmanager
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   environment:
  #     <<: *env-vars
  #     ENABLE_BUILT_IN_PLUGINS: flink-python-2.0.0.jar;flink-s3-fs-hadoop-2.0.0.jar
  #   volumes:
  #     - ./docker/flink/flink-conf.yaml:/opt/flink/conf/config.yaml:ro
  #   ports:
  #     - "8082:8081"       # Flink Web UI
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 12
  #   command: jobmanager

  # flink-taskmanager-1:
  #   <<: *common-settings
  #   build:
  #     context: .
  #     dockerfile: docker/flink/Dockerfile
  #   container_name: flink-taskmanager-1
  #   depends_on:
  #     flink-jobmanager:
  #       condition: service_started
  #   environment:
  #     <<: *env-vars
  #     ENABLE_BUILT_IN_PLUGINS: flink-python-2.0.0.jar;flink-s3-fs-hadoop-2.0.0.jar
  #   volumes:
  #     - ./docker/flink/flink-conf.yaml:/opt/flink/conf/config.yaml:ro
  #   command: taskmanager

  # flink-taskmanager-2:
  #   <<: *common-settings
  #   build:
  #     context: .
  #     dockerfile: docker/flink/Dockerfile
  #   container_name: flink-taskmanager-2
  #   depends_on:
  #     flink-jobmanager:
  #       condition: service_started
  #   environment:
  #     <<: *env-vars
  #     ENABLE_BUILT_IN_PLUGINS: flink-python-2.0.0.jar;flink-s3-fs-hadoop-2.0.0.jar
  #   volumes:
  #     - ./docker/flink/flink-conf.yaml:/opt/flink/conf/config.yaml:ro
  #   command: taskmanager

  # flink-submit:
  #   <<: *common-settings
  #   build:
  #     context: .
  #     dockerfile: docker/flink/Dockerfile
  #   container_name: flink-submit
  #   depends_on:
  #     flink-jobmanager:
  #       condition: service_healthy
  #   environment:
  #     <<: *env-vars
  #     ENABLE_BUILT_IN_PLUGINS: flink-python-2.0.0.jar;flink-s3-fs-hadoop-2.0.0.jar
  #   volumes:
  #     - ./src/streaming/consumer:/opt/flink/scripts
  #     - ./docker/flink/submit_all.sh:/opt/flink/submit_all.sh:ro
  #     - ./docker/flink/flink-conf.yaml:/opt/flink/conf/config.yaml:ro
  #   #command: ["bash","/opt/flink/submit_all.sh"]
  #   command: ["bash","-c","/opt/flink/submit_all.sh; sleep infinity"]

volumes:
  postgres-airflow_metadata:
